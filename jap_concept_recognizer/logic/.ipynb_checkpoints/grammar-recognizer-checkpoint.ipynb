{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuromojipy.kuromoji_server import KuromojiServer\n",
    "kuro_server = KuromojiServer()\n",
    "from japanese_text_extractor.japanese_text_extractor import extract_japanese_text\n",
    "from functools import reduce\n",
    "from grammar_recognizer import encode_tags\n",
    "import romkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import concept_mapper\n",
    "importlib.reload(concept_mapper)\n",
    "\n",
    "mapper = concept_mapper.ConceptMapper(kuro_server, \"../id_reference.csv\", \"293\", \"490\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end!\n"
     ]
    }
   ],
   "source": [
    "sents = {}\n",
    "with open(\"sentences.csv\", \"r\") as file:\n",
    "    next_line = \"asdf\"\n",
    "    while next_line != \"\":\n",
    "        next_line = file.readline()\n",
    "        splited = next_line.split(\"\\t\")\n",
    "        if len(splited) > 1 and splited[1] in [\"jpn\", \"eng\"]:\n",
    "            sents[splited[0]] = splited[1:]\n",
    "        if next_line == \"\":\n",
    "            print(\"end!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "with open(\"jpn_indices.csv\", \"r\") as file:\n",
    "    next_line = \"asdf\"\n",
    "    while next_line != \"\":\n",
    "        next_line = file.readline()\n",
    "        splited = next_line.split(\"\\t\")\n",
    "        if len(splited) == 3:\n",
    "            jp_id, eng_id, text = splited\n",
    "            if jp_id in sents and eng_id in sents:\n",
    "                translations.append([sents[jp_id][1].strip(), sents[eng_id][1].strip(), text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing():\n",
    "    missing = {}\n",
    "    good = 0\n",
    "    for sent in all_sents[:500]:\n",
    "        result = mapper.analyze_sent(sent)\n",
    "        if result[\"missing\"] != []:\n",
    "            for elem in result[\"missing\"]:\n",
    "                if elem[0] == \"から\":\n",
    "                    print(sent)\n",
    "                if elem[0] in missing:\n",
    "                    missing[elem[0]] += 1\n",
    "                else:\n",
    "                    missing[elem[0]] = 1\n",
    "        else:\n",
    "            good += 1\n",
    "    print(good/500)\n",
    "    print(list(sorted([(x, missing[x]) for x in missing.keys()], key=lambda x: x[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_concepts_to_id(concept):\n",
    "    if type(concept) == concept_mapper.WordConcept and type(concept.produce) == list:\n",
    "        return [x.id for x in concept.read]\n",
    "    return [int(concept.read.id) if type(concept) == concept_mapper.WordConcept else int(concept.id)]\n",
    "\n",
    "def concept_to_exp(exp):\n",
    "    jap_texts = extract_japanese_text(exp)\n",
    "    results = [mapper.analyze_sent(x) for x in jap_texts]\n",
    "    concepts = reduce(lambda acc, curr_value: acc + [*curr_value[\"found_words\"], *curr_value[\"found_grammars\"]], results, [])\n",
    "    return list(set(concepts))\n",
    "\n",
    "def concepts_to_sent(results):\n",
    "    return reduce(lambda acc, curr: acc + [str(x) for x in sent_concepts_to_id(curr)], [*results[\"found_words\"], *results[\"found_grammars\"]], list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concepts_to_id(concepts):\n",
    "    return [int(x.read.id) if type(x) == concept_mapper.WordConcept else int(x.id) for x in concepts]\n",
    "\n",
    "def create_exp_csv():\n",
    "    exp = open(\"exp.txt\", \"r\").read()\n",
    "    exp = exp.replace(\"\\n\", \" \").replace(\";\", \",\")\n",
    "    concepts = concept_to_exp(exp)\n",
    "    concept_ids = concepts_to_id(concepts)\n",
    "    with open(\"exp.csv\", \"w+\") as file:\n",
    "        file.write(exp+\";\"+\" \".join(concept_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'The が identifier particle', 'coords': [{'start': 1, 'end': 3}]}, {'name': 'i-adjectives', 'coords': [{'start': 2, 'end': 4}]}, {'name': 'i-adjectives/conjugation', 'coords': [{'start': 2, 'end': 5}]}, {'name': 'verbs', 'coords': [{'start': 0, 'end': 1}]}, {'name': 'The target に particle', 'coords': [{'start': 6, 'end': 8}]}, {'name': 'using masu', 'coords': [{'start': 8, 'end': 10}]}]\n",
      "[33558, 45486, 34390, 45539, 45568, 45512, 33844]\n"
     ]
    }
   ],
   "source": [
    "print(concepts_to_id(concept_to_exp(\"時間がなかったからパーティーに行きませんでした。\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sent_cards_csv():\n",
    "    sents = [[y.strip() for y in x.split(\";\")] for x in open(\"sents.csv\", \"r\").readlines()]\n",
    "    lines = []\n",
    "    for line in sents[:100]:\n",
    "        results = mapper.analyze_sent(line[0])\n",
    "        if results[\"missing\"] == []:\n",
    "            lines.append([\"How would you say in Japanese? <br /><h1>{}</h1>\".format(line[1]), \"<h1>\" + line[0] + \"</h1>\", \" \".join(concepts_to_sent(results))])\n",
    "    with open(\"outcards.csv\", \"w+\") as file:\n",
    "        file.write(\"\\n\".join([\";\".join(x) for x in lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_many_sent_cards():\n",
    "    sents = translations\n",
    "    lines = []\n",
    "    i = 0\n",
    "    for line in sents[:10000]:\n",
    "        i += 1\n",
    "        try:\n",
    "            results = mapper.analyze_sent(line[0])\n",
    "            if results[\"missing\"] == []:\n",
    "                lines.append([\"What does this mean? <br /><h1>{}</h1>\".format(line[0]), \"<h1>\" + line[1] + \"</h1><span>({})</span>\".format(\"\".join([romkan.to_hiragana(romkan.to_roma(x[-1])) for x in mapper.pos_tag(line[0])])), \" \".join(concepts_to_sent(results))])\n",
    "        except:\n",
    "            print(line)\n",
    "    with open(\"outcards_3.csv\", \"w+\") as file:\n",
    "        file.write(\"\\n\".join([\";\".join(x) for x in lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text_in_color(text, color):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_analyzed_sent(sent):\n",
    "    pos_tags = mapper.pos_tag(sent)\n",
    "    analyzed = mapper.analyze_sent(sent)\n",
    "    words = [x[2] for x in pos_tags]\n",
    "    parts = [(x[0], x[1], x[1] + 1) for x in analyzed['verbose_found_words']] + [(x[0], x[1][0]['start'], x[1][0]['end']) for x in analyzed['verbose_found_grammars']]\n",
    "    parts = [x for x in parts if x[1] != x[2]]\n",
    "    colors = [\"red\", \"green\", \"blue\", \"purple\"]\n",
    "    i = 0\n",
    "    for part in parts:\n",
    "        words[part[1]:part[2]] = [show_text_in_color(x, colors[i%len(colors)]) for x in words[part[1]:part[2]]]\n",
    "        parts[i] = [*part, colors[i%len(colors)]]\n",
    "        i += 1\n",
    "    display(html_print(\"<h1>\" + \"\".join(words) + \"</h1><br />\" + \"<br />\".join([\"{} - {}\".format(x[0], show_text_in_color(x[3], x[3])) for x in parts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
